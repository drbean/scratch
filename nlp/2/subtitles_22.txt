
Let's introduce the noisy channel model of
spelling. The intuition of the noisy
channel, and it comes up throughout
natural language processing, is that we
have some original signal. Let's say it's
a word. And we imagine that it goes
through some channel. And the idea was
originally invented for speech, where
there were, you know, if you talk into a
tube or you go over some kind of
telecommunications line, and the word is
distorted. And so what comes out from the
original word, is some noisy word. And
we've represented that here with a weird
font. But. In the spelling case we imagine
that, oh somebody mistyped the word. So
the channel is the typewriter or the
person typing or the keyboard, and at the
end, you've got a misspelled version of
the word. And our goal in the noisy
channel is to take that output, of that
noisy process, and by modeling how this
channel works, we built a model,
[inaudible] model of the channel. We can
run all possible original words through
that channel and see which one looks the
most like the noisy word. So the decoder
will take. A bunch of hypotheses. For each
one, run it through the channel, just
running hypothesis two through the
channel, run hypothesis three through the
channel, and we see which word looks the
most like this noisy word, and we pick
that as the original hypothesis for the
word that started out. So let's look at
that. First we'll introduce some
probability and then we'll look at some
examples. The noisy channel is a
probabilistic model. Our goal given an
observation x of some misspelling. Some
word we've seen, some surface thing we've
seen, some observation x. We'd like to
find w the correct word. And we're gonna
model that. Probabilistically, by saying
we're looking. The best word, the word
that we'd like to replace our misspelling
with is that word out of the vocabulary
that maximizes a probability. What
probability? The probability of the word
given the misspelling. So what word, given
that we've seen some misspelling. What's
the most likely word? The most probable
posterior probable word, given that
misspelling. And we're gonna use
[inaudible] rule, to re-, to replace that
probability. So, the probability of W
given X. We're gonna replace that with P
of X, given W. P of W over P of X. And so,
we, we, we can also eliminate the
denominator. So whatever word maximizes
this equation. We'll also maximize this
equation. We're asking, given a
misspelling X, what's the most likely
word? And since the formula for that
probability includes the probability of
the word, the misspelling X. We're
including that probability in every W that
we're considering. So if sum W, say, W
hypothesis one, has a greater probability
than hypothesis two. By this equation,
it'll also have a greater probability by
this equation, because X is a constant. X
is the misspelling that we're trying to
decide if W1 or W2 is a better hypothesis
for it. So that means. That the noisy
channel model. Comes down to maximizing
the product of two factors. The
likelihood. And the prior. And we
generally call this term the language
model. And you've seen language models
before. That's the probability of the
error, probability of, excuse me, that's
the probability of the correct word, w.
And this likelihood term, we often call
this the channel model. Where sometimes
the error model. So we've got two factors.
The language model and the channel model,
and the intuition is. The language model
tells us how likely would this word be to
be a word perhaps in this context, perhaps
by itself. The channel model says, well,
if it was that word, how likely would it
be to generate this exact error? So the
channel model was sort of modeling that
noisy channel that turns the correct word
into the misspelling. Now this noisy
channel model for spelling was proposed
around 1990. Independently of two separate
laboratories. And the use of speech
recognition models like noisy channel came
into natural language processing right
around then. Mainly, although not
exclusively, because of the work at these
two labs at IBM, and at, AT and T Bell
Labs. And so the examples we're gonna take
for the rest of this example come from,
these two, important early papers by
[inaudible] and by Khernigan [inaudible].
So let's look at an example. Here's a
misspelling. The word A-C-R-E-S-S. So
think for yourself for a second what this
could mean. First, we're gonna start with
generating candidates. What are the
possible candidate words to replace this
word? And we can think of a, at least a
couple of obvious ways to do this, one is,
we're gonna pick words that have similar
spelling. So words that, that are, have
similar spelling might naturally be
mistaken for, for the correct word. And
we're gonna. Operationalize similar
spelling as having a small edit distance
to the error. Or we could pick words with
similar pronunciation and there we're
gonna pick a word with a small edit
distance of the pronunciation to the
error. And we're gonna, for the rest of
this example I'm gonna pick the first
approach. So, we're gonna pick words that
have similar spelling as our possible
candidates. How do I operationalize
similar spelling? Well, we've seen edit
distance before. And remember, with edit
distance we talked about the distance
between two strings, the minimal number of
edits that turns one string into another,
where we define an edit as an insertion, a
deletion, or a substitution, so any of
these three. For spell correction, we're
gonna want to add a fourth possible edit
operation, transposition, because in
practice for spelling errors, we often
transpose two letters. And that version of
edit distance is now called Damero
Levenstein edit distance. And it can be
computed, again by various dynamic
programming approaches. So let's look at
the candidates that are words within a
distance one of our misspelling
A-C-R-E-S-S. So here's our error.
A-c-r-e-s-s and here is different possible
candidates. So here's a candidate actress.
How is actress turned into across? Well,
the T turns into nothing, so a T was
deleted. So we have a deletion of a T. So
a deletion of a T turns actress into
across. Here, the proposed candidate is
the word cress. The kind of vegetable. So,
here crest, to turn crest into aquarist we
have to insert add, insert an a. So, here
we had a deletion, here we had an
insertion. How about caress? Caress is to
turn caress into aquarist we turn a ca
into the ac. We have a transposition of ca
and ac. The word could've been access.
Here we have a substitution: the c turned
into an r. Or another substitution. The
word could've been across, and the o
turned into an e. Or an s could've been
inserted, to turn acres into, into acres?;
but the s could've been inserted either
here. Or here, so there's two different
ways where this source word could have
turned into this error form. So we'll put
two rows down for both of these possible
insertion locations, positions. So I've
just shown you candidates that are within
ETA distance of one. It turns out that,
that 80 percent of, of spelling errors are
within ETA distance of one. And almost all
errors are within ETA distance of two. So
most algorithms either consider just ETA
distance one or ETA distance two possible
candidates. In practice we also want to
allow, not just insertion and substitution
of letters, but also of spaces or hyphens.
So for example, if the user types this
idea, we'd like to realize that, there
should be insertion of a space, or that
the original space was in fact deleted to
produce this error form. Or here, the
original dash in the word in-law was
deleted this error form, in-law. We've
seen candidate generation, now we're ready
to talk about how to rank the candidates.
And remember, there are two factors. We
have the language model and the channel
model. Now, the language model, we can use
any of the language modeling algorithms
we've already learned. We can use unigrams
and bigrams and trigrams. We can use any
kind of back off algorithm we wanna use,
or smoothing algorithm we wanna use, in
practice for very, very large scale web
scale correction, we're gonna use, as
usual, for web scale things, we're gonna
use stupid back off. But we might wanna
use smarter algorithms for smaller kinds
of tasks. [sound] So let's look at an
example of a language model. Here I picked
just a very simple unigram. And in this
case we've computed the unigram from the
corpus of contemporary English, one of the
many possible corpora. And here's some
counts. Here's counts of the different
possible candidates actress, cress,
caress, and so on. Here's their frequency
and normal life by the total number of
words we get a probability of the total
number of words. We get a normalizing
discount by the total count, we get
probabilities. So here's the probabilities
of, of words assigned by unigram language
model. How about computing the channel
model probability. Remember, the channel
model's also called the error model or the
edit probability. And we're gonna take a,
simplifying assumption made by, Kernighan,
Church, and Gale in 1990, when they first
proposed the use of the noisy channel
models. So, let's first see how to do
that. Let's assume that a misspelled word
X has a set of letters, X1 through XM. And
the correct word, W, has a set of letters,
let's call them W1 through WN. Now the
probability of the edit X, given W. Is
gonna be some set of deletions or
insertions or substitutions or
transpositions, some set of edits. The way
we're gonna model that is we're gonna
create a confusion matrix. And a confusion
matrix says for any given letter pair of
letters, how likely is a particular edit
to happen. So for example, for the pair of
letters X, Y. We want to know how often xy
is typed as x. Meaning, how often is a y
delete when there's a x right before it.
We're gonna also keep a count of for
insertion probabilities how often was an X
typed as XY. So how often is Y inserted
after X. So Y deleted after X, Y inserted
after X. Or we'll keep a count for
substitutions. How often is X typed as Y?
So we meant to type X, we typed Y. That's
an XY substitution. Or a transposition,
how often was XY typed as YX? So these are
just counts. We'll keep a matrix of these
counts for every X and for every Y. I
noticed what we've done implicitly is
we've conditioned our insertion. And our
deletion on the previous character. So
whether Y is deleted is conditioned on X.
We could have conditioned twelve from the
condition of the next character or the
character five to the left or some other
thing, but we generally condition on the
previous character. So here's an example
of a confusion matrix for spelling errors.
The font is a little small, but just to
give you a basic idea. Here's this is a
substitution matrix that I took from
Kernigan et al. So here's the letter e,
and it's very likely in their, in their
data 388 times to be substituted with an
a. So, you meant to type e, you
incorrectly typed an a. Or you might have
typed an I, or you might have typed an o.
So vowels are very likely to be mist,
mistaken for each other. Or similarly. The
letter m very often gets mistyped as an n.
So, a very high probability of m and n
being substituted for each other. They're
next to each other on the keyboard. They
sound alike. Lots of reasons for them to
be substituted. So, here's our set of
confusion matrices, and we just compute
four of them. One for substitution. One,
substitution. One for insertion. One for
deletion. And one for transposition. Now
I've shown you this table comes from
[inaudible]. But you could also generate
the table yourself. So for example Peter
Norvick post on his website a lovely list
of errors. So these are errors taken from
Wikipedia and other places, that, he talks
about on his website. And from a set of
errors like this. So, here, misspellings
of adaptable as, as, adaptable or,
immature with only one M, and so on. So
various kinds of likely misspellings. And
from this list of errors we can get a list
of counts for every possible single error,
single edit error of how often it happens
and from that we can build. So we build
our little confusion matrix and then from
the confusion matrix we can generate
probabilities. So. Every time a particular
previous letter happens. We, we look up in
our insertion may confusion matrix and we
say how often was xi inserted after a
particular letter w sub I minus one and we
divide by the number of times we minus one
occurred and that's gonna be the
probability of a particular insertion.
Happening, in a word. So we can generate
our probability of our surface form by for
each possible single edit error, again
we're assuming a single edit now, so one,
only one of these happens to generate our
candidate. Whichever one it is, we compute
our probability by just normalizing the
count of the deletion or insertion or
substitution or transposition, by the
appropriate count, and generate a
probability. So, this channel model. For
example for a word like actress. Where we,
we generated A-C-R-E-S-S by when we should
have typed a C. Excuse me. When we should
have typed a CT, we typed a C so the word
had a CT in it but the error had only a C.
So what's the probability of deleting a T
following a C? And if we'd normalized the
probabilities in our confusion matrix,
here's the likelihood of this word actress
being realized as the misspelling acres?,
it's.000117. The language model so, here's
the error model or the channel model. And
now we can add in the language model, or
write LM. So we have the channel model.
How likely was CT to be, error fully
turned into C? So T to be deleted. And how
likely is the word actress, anyway? And we
can just multiply these together. And what
we'll do is, because these are very small
numbers, we'll just multiply everything by
ten to the ninth to, to make it readable.
So, so this would be 2.7 times ten to the
minus ninth. But we multiplied everything
by ten to the ninth here. So you can see
that the most likely word here is across.
I, with this particular this particular
channel model, and this particular
language model the most likely word is a
cross. But, actress is also quite likely.
And, and acre seems a reasonably
likelihood. And the word crass, which is
just a very rare word, you can see, has a
very low probability. And has an unusual
error of inserting an A at the beginning.
Makes it a very low probability
correction. So the noisy channel model
like the word across as the possible
replacement. Unfortunately. We can see
from the original sentence, taken from
Kernighan et al's paper, that the original
sentence across is the wrong word. The in
the original sentence is a stellar and
versatile. [inaudible] whose combination
of sass and glamour. And it should be
clear that this word should have been
actress. So it crosses the wrong word. So,
just using a unigram model, the noisy
channel makes a mistake. So let's look at
a bigram model. How well can we do with a
bigram model? So we computed a very simple
bigram model, just using [inaudible], add
one smoothing from the corpus of
contemporary American English. So now, the
probability of actress given versatile.
Just look at these three words, and ignore
the rest for now. Actress, given
versatile. That probability is.00021, and
who is given actress is.00010 so we'll
compute those. And now let's do the same
thing for another candidate, the original
candidate that was preferred by the
unigram model, the word cross. We'll put a
cross here, instead is our hypothesis, and
again we'll compute the probability of a
cross giving versatile times the
probability of who is giving the cross. So
here's those probabilities. And you can
see that the probability of who was given
actress is much higher than the
probability of who was given across.
Actress who is just a likely sequence. And
sure enough, if we multiply these things
out, the probability of versatile actress
who's is a much higher probability than
the prob-, then the sequence versatile,
across whose. So a much higher
probability. So the noisy channel model
with a bigram language model correctly
picks the, correction actress. How are we
gonna evaluate the these noisy channel and
other kinds of models? There are lots of
good spelling error test sets. Wikipedia
has a list of common English misspellings.
There's a filtered version of that at A
spell. There's a spelling error corpus at
Birkbeck. Let's look at the Wikipedia
list. So there's Wikipedia's list of
common English misspellings. And, I've
shown you here on this slide some various
other possible lists that you can go look
at on your own. So from these lists of
misspellings you would generate a training
set to train your channel model. A
development set to test out your model and
then a final test set to see how well your
model works. So that's the noisy channel
model of spelling applied to, to non-real
