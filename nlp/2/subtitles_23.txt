
We've seen how to correct non-words of
English but what happens if the error
produces a real word? This turns out to be
a very common problem. Maybe between a
quarter and a half of spelling errors
depending on the application turn out to
be real words. So in these examples from,
from a classic paper by Karen Kukich. The
word minutes is misspelled as minuets,
perfectly resembling English word. The
word and is misspelled as the word an, a
very, a very common English word. Leave as
lave, by as be, and so on. So these are,
not only are these English words, but some
of them are quite common, and, and
frequently used English words. So a much
tougher problem, solving the real word
spelling error, task. Again what we're
gonna do for the real word spelling error
is very similar to what we did for the,
the non real words, we're gonna generate a
candidate set, which includes the word
itself and all single letter [inaudible],
that produce English words and may be
we'll also in some version produce,
includes word that are [inaudible], sound
to like and giving this [inaudible] set
for each word. We'll choose the best
candidates. Either using the noisy channel
model or you can imagine we'll talk later
about more, complex models that use
classifiers. So let's look at that in
detail. Given a sentence with word one
through word N. We're going to generate,
for each word, a set of candidates. So for
word one, we have the candidates word one
itself, and then a bunch of varies, the
single edit distance neighbors of that
word. Word one prime, word one double
prime, word one triple prime, word two,
word two prime, word two double prime,
world two triple prime, and so on for each
of the words. So we have a whole lot of
candidates for each of the words. And now
we're gonna choose the sequence, capital
w, the sequence of candidates that
maximize, has a maximal probability. In
other words, we might pick. Word one from
this candidate set. And word two prime,
prime from this candidate set and word
three prime, prime, prime from this
candidate set and so on. For each word
we're gonna pick some candidate, which
might be the word itself or some
correction of that word. And we're gonna
pick the sequence that is most likely.
Let's look at an example of that. We have
the imagine the three words, the mini, the
mini sentence, two of, thew, thew, thew.
So for each word, the word two, the word
of, and the word [inaudible], we generate
potential corrections, each of which is a
word of English that is [inaudible] of
one. So, I've shown some here. So, two
could have been the word T-O, if the word,
if the original word two was the error was
an insertion of a W. Or, it could have
been the word tao, where the error was a
substitution of A for W. Or, it could have
been the word too, substitution of an O by
a W. Or it could have been correctly, the
word two could be correct. Similarly of,
could have been, the correct word could
have been off, and there was a deletion of
an F. So, again, three candidates off, on,
and including the word of itself, and the
word few, which is a real word of English.
Could have been the word through, and the
r got deleted, or the word thaw, or the
word the. A very common word, and, ew, a
very likely error, it turns out. Because W
is right next to e in the keyboard, and so
on. And so we have each of our candidate
sets. And then we just wanna ask, of all
the possible sets of sentences. Produced
by paths in this, in this graph. So here's
one. Two of through. Here's another one.
Two on thaw. Here's another one. Two of
the. And so on, for each of those,
possible sentences, what's the most likely
one according to the noisy channel. We
pick the mo, excuse me, the most probably
one according to the noisy channel. And
hopefully the, the noisy channel, a good
noisy channel model will predict the, will
pick the correct answer to of the, as our
most likely sequence here. In practice,
for spelling correction, we often make the
simplification of we're only seeing one
error, rather than letting every word have
a, have a possible error in it. In other
words. The set of sequences we consider is
the sequences in which only one of the
words is an error. And the rest of the
words were correct as typed. So here, word
one, word three, and word four were
correct as typed, and it was word two that
was misspelled, and we replace it by word
two, double-prime, let's say. Or, in this
sequence, it was the word three that was
misspelled. Thew was misspelled as, the
was misspelled at thew and so here's the
mis, the error and these, these three
words are correct and so on. And so this
smaller set of possible candidate
sequences. So instead of having to
consider N squared possible sequences
we're just considering a constant times n
possible sequences. From this set now we
choose the sequence that maximizes, that
has the maximum probabilities that we
picked. The, the most likely, the most
probable most conditionally, most probable
set of sequence, sequence of candidates.
[sound] Where do we get these
probabilities? Again, we can the language
model. Just as we saw before, we have our
unigram, we have our bigram. We can use
whatever smoothing method we'd like. The
channel model's just the same, again, as
for the non word spelling error
correction. The only difference is, we now
need a probability of having no error.
Because, of course, we're assuming that
only one of the words is an error. So, we
have, we have to have a probability for
all those other words that are not an
error. We need to be able to, to, decide,
when we have an error, and when a word is,
in fact, correct. Meaning that the
probability of the word itself, given the
word, is high, so, unlikely to have an
error. How do we compute this probability
of no error? What's the channel
probability for a correctly typed word?
And this obviously depends on the
application. And so we might make the
assumption, that, in a particular
application, you know, one word one ten is
typed wrong. And that means that the
probability of, of, a correctly typed word
is.9. Or we might have, instead, the
assumption that one word in 200 is wrong.
And so now, the probability of, of any
word being typed correctly is.995. So
there's our channel model probability of a
word not changing. Let's assume that the
channel model of a task is has a
probability of one in twenty of an error.
Meaning that 95 percent of the time a word
is correct as typed. So, here's an example
from Peter Norvig. Again we have the
spelling error, thew. We want to know
whether it should be the word the. The
word few, correct, that it was correct as
typed or thaw or through or three and so
on. And again, for each one, we generate
our channel model and the other channel
models were exactly computed the same way
as before. We have the probability of a
substitution of A, subset of a, A, being
substituted by a E or of an R being
deleted after an H and so on that we can
compute just from our channel models. Well
here's our channel model probabilities.
And again, we have our language model
probability, just as before. And these are
examples that Pierre Norbert computed from
the Google anagram counts. And again we've
assumed the channel model of a word not
changing, of the, of the error X being
generated by correct, correctly generated
by the word X. And we can multiply these
together, multiply together the channel
model with the language model and again
showing you these multiplied by ten to the
ninth to make it easy to read. You can see
that the word the. Is correctly chosen,
very high probability as the misspelling
of the word the by itself and in context,
and this is using a unigram language
model, if we're using a bigram or a
trigram or even more likely probably to be
able to distinguish when the word the
really was the word the and when it was
the word the. So that's a real word
spelling correction, we simple take the,
the standard algorithm, noisy channel
algorithm, for non real words, add an, a
probability of edit not happening and then
allow every word to imagine every word
could have been an error and then look for
the most likely sequence, simplifying
usually by [inaudible], one error per
