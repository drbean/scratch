
Let's talk about cases where we need to
interpolate be, between or back off from
one language model to another one. And
we'll also touch on the web today. These
are cases where it helps to use less
context rather than more. And the
intuition is, suppose that, you have a, a
very confident trigram. You've seen a
trigram a very large number of times.
You're very confident this trigram is a
good estimator. Well we should use the
trigram. But suppose you only saw it once.
Well maybe you don't really trust that
trigram. So you might want to back off and
use the bigram instead. And maybe you
haven't seen a bigram either. You might
back off to the unigram. So he idea of
back off is, sometimes if you don't have
a, a large count or a trustworthy evidence
for a larger order enneagram, we might
back off to a smaller one. A related idea
is interpolation. Interpolation says, well
sometimes the trigram may not be useful
and in that case, if we mix trigrams and
bigrams and unigrams, well then we may get
more information from the unigrams and
bigrams but other times trigrams will be
more useful and so interpolation suggests
that we just mix all three all the time
and, and, and get the benefits of all of
them and it turns out in practice that
interpolation works better than back off.
So most of the time in language modeling,
we'll be dealing with interpolation. There
are two kinds of interpolation. Simple
linear interpolation, we have, our
unigram, our bigram, and our trigram. And
we simply add them together with three
weights... Lambda one, Lambda two, and
Lambda three. The Lambdas just sum to one
to make this a probability. And, and, and,
and we can compute our new probability.
We'll call it P hat of a word given the
previous two words, by, by interpolating
these three, language models. We can do
something slightly more complicated. We
can condition our lambdas on the context.
So we can say, still mix our trigram or
bigram with a unigram but now, the lambdas
are dependent on what the previous two
words were. So we can train even a richer
and, and more complex context conditioning
for deciding how to mix our trigrams and
our bigrams and our unigrams. So, where do
the lambdas come from? The normal way to
set lambdas is to use a held out corpus.
So we've talked before about having a
training corpus. Here's our training
corpus and our test corpus. A held out
corpus is yet another piece that we set
out, set aside from our data. And we use a
held out corpus. Sometimes we, we use, a
held out corpus called a dev set. A
development set, or other kinds of held
out data. We use them to set
metaparameters and check for things. So in
this, we can use the held out corpus to
set our Lambdas. And the idea is, we're
gonna choose Lambdas which maximize the
likelihood of this held out data. So
here's what we do. We take our training
data, and we train some enneagrams. Now,
we say, which Lambdas would I use to
interpolate those enneagrams, such that,
it gives me the highest probability of
this held out data. So we, we ask, find
the set of probabilities, such that the
log probability of the actual words that
occur in the held out data are highest.
Now we've talked about cases where there
is zeros, so we haven't seen some bi-gram
before and we have to replace that zero
count with some other count, that's
smoothing. But what do we do if the actual
word itself has never been seen before.
Now sometimes that doesn't happen. In
tasks where, let's say a menu based task,
where we're, where we have a thick set of
commands, then no other words can ever be
said. Our vocabulary is fixed, and we have
a, what's called a closed vocabulary task.
But, lots of times, language modeling is
applied in cases where we don't know, any
word could be used and it could be words
we've never seen in our training set. So
we call these words OOV or out of
vocabulary words. And one way of dealing
with out of vocabulary words is as
follows, we create a special token called
unk. And the way we train unk
probabilities is we create a fixed
lexicon. So we take our training data and
we first decide which, we hold out a few
words, the very rare words or the
unimportant words, and we take all those
words and we change those words to unk.
Now we train the probabilities of unk like
a normal, any normal word. So we have our
corpus, our training corpus. It has word,
word, word, and it has a really low
probability word, word, word, word, and
we'll take that word and we'll change it
to unk. And now we train out bigram word,
word, word unk word, word, word as just as
if unk had been a word in there and now at
decoding time if you see a new word you
haven't seen you replace that word with
unk and treat it like get its, its bigram
probabilities and its trigram
probabilities from the unk word in the
training set. Another important issue in M
grams has to do with web scale or very
large M grams. So we introduced the Google
M grams corpus earlier. How do we deal
with computing probabilities in such large
spaces? So, one answer is pruning. We only
store n-grams that have a very large
count. So for example the very high order
N grams we might want to remove all of
those singletons. All of the things with
count one because by Ziff's law there's
gonna be a lot of those singleton counts.
And we can also use other kinds of more
sophisticated versions of this. We don't
just remove things with counts we actually
use compute the [inaudible] perplexities
on a test set and remove counts that are
contributing less to the probability on a
particular held out set. So that's
pruning. We can do a number of other
efficiency thing. We can use efficient
data structures like tries. We can use
approximate language models which are very
efficient but are not guaranteed to give
you the exact same probability. We can, we
have to do efficient things like don't
store the actual strings but just store
indexes. We can use Huffman coding and
often instead of storing our probabilities
as these big 8-byte floats, we might just
do some kind of quantization and just
store a small number of bits for our
probabilities. [sound] What about
smoothing for web scale enneagrams. Most
popular smoothing methods for these very
large enneagrams is an algorithm called
Stupid Back off. Stupid Back off is called
stupid because it's very simple but it
works well at the very large scale. And
the fact it's been shown to work as well
as any more complicated algorithm when you
have very large amounts of data. I mean
intuition of Stupid Back off is if I wanna
compute the Stupid Back off probability of
a word given some previous set of words. I
use the maximum likelihood estimator and
this is the count of the words divided by
the count of the prefix, if that count is
greater than zero, and if not, I just back
off to the, to the probability of the
previous. The lower order n gram prefix
with some constant weight, so it's if, if
the trigram would say occurs I just use
the count of the trigram, if it doesn't I
take the bigram probability and multiply
it by point four and just use that. And
then when I get down to the unigrams if I
don't have anything at all I just use the
unigram, I just use the, the unigram
probability, so, We call this S. Instead
of P, because stupid back off doesn't
produce probabilities because to produce
probabilities we would actually have to
use various clever kinds of waiting a back
off algorithm has to discount this
probability to leave some mass left over
to use the bigram probabilities.
Otherwise, we're gonna end up with numbers
that are greater than one, and we won't
have probabilities. But, but, so stupid
back off produces something like scores,
or, or, rather than, than, probabilities.
But it turns out that this, this works
quite well. So, in summary, for smoothing
so far, add one smoothing is okay for text
categorization, but it's not recommended
for language modeling. The most commonly
used method we'll discuss in the advanced
section of this week is, the, the
[inaudible] Nye algorithm, or the extended
interpolated [inaudible] Nye algorithm.
But for very large enneagrams, like
situations where you're using the web,
simplistic algorithms like stupid back off
actually work quite well. How about
advanced language modeling issues? Recent
research has focused on things like
discriminative models. So here, the idea
is, pick the enneagram weights. Instead of
picking them to fit some training data,
whether it's maximum likelihood estimate
or smooth. Instead, choose your enneagram
weights that improve some task. So we'll
pick a, whatever task we're doing, machine
translation or speech recognition, and
choose whatever enneagram weights make
that task more likely. Another thing we
can do is instead of just using anagrams,
we can use parsers. And we'll see the use
of parsers and statistical parsers later
in the course. Or we can use caching
models. In a caching model we assume that
a word that's been used recently is more
likely to appear again. So the
probability, the cache probability of a
word, given some history, we mix the
probability of the word. With some
function of the history, how, like, how
much, how often the word occurred in the
history with, and we, we weight those two
probabilities together. It turns out that
cache models don't work in certain
situations, and in particularly, they
perform poorly for speech recognition. You
should think about why that might be, that
a cache model performs poorly for speech
