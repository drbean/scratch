
There are a few more issues that come up
in spelling correction that we wanna
include in any kind of state-of-the-art
system. One is HCI issues, Human Computer
Interaction issues. So if we're very
confident in the correction for example,
we might wanna auto-correct. And so that
happens very often as I talked about
earlier with the example "hte" which is a
very common misspelling of "the". If we're
slightly less confident, we might wanna
give a single best correction. But to the
user to just, to, to say yes or no to. If
we're even less confident, we might wanna
give the user a whole list, and let them
pick from this list. And if we're just
unconfident at all but we're pretty sure
we saw an error, we just don't know how to
fix it then we might just flag what the
user typed as an error. So various things
again depending on our application, and
depending on the probability and the
confidence value that we might generate.
In practice for almost all noisy channel
models. Even though we, we define that
model as multiplying a prior and a, and a
likelihood and a error model. In practice,
these two probabilities are computed from
with, making a lot of independence
assumptions about how many errors there
were, and, and the fact that the spelling
is independent of neighboring words, and
these are really not true. And the result
of these incorrect independence
assumptions means that these two
probabilities are often not commensurate.
So we do, in fact, is, instead of just
multiplying these two, we weight them. And
the way, since we're multiplying
probabilities, we weight them by raising
one of them to a power. We can't,
obviously, multiply one of them by
something. So we, we weight them by
raising one of them to a power lambda. And
we learn this lambda from some development
tests that we pick whatever lambda to, to
raise the, the language multiple
probability to such that the product is
more likely to pick out just those errors
that really are errors. And we use this
weighting of the, of the noisy channel
model in almost any application that we
see with a noisy channel model. Something
else that's used in the state of the art
systems is to use not just the spelling
but the pronunciation of the word to help
us find errors. So the metaphone system
which is used in, in GNU Aspell instead of
just asking for candidates that have a
[inaudible] a similar spelling, ask for
candidates that have a similar
pronunciation. And that's done by first
converting the misspelling to a
pronunciation. And, and the metaphone is a
simplified pronunciation system, that, a
set of rules that convert a word into a,
something approximating a pronunciation.
And here's the rules that get used. Drop
duplicate adjacent letters, except for C.
If the word begins with kn, or gn, drop
that first letter. Drop b if it's after an
m and if it's at the end of the word and
so on. These are dropping various silent
letters and various rules like this
convert the Misspelling into a kind of a
representation of the pronunciation, as a
single vowel at the beginning and then a
set of consonants. And then we find words
whose pronunciation is nearby the
misspellings pronunciation, so we've
converted all other words, into the, into
the metaphone pronunciation. Find similar
words and now. We score the words by, by
some combination of two ETA distances. How
likely is the candidate to be
orthographically changed into the
misspelling, so we'll use some kind of
channel model like thing. And the same
thing with the pronunciation. How likely
is the misspelling to be pronounced like
the candidate. So a metaphone system
doesn't use a language model. But use them
pronunciation-based kind of channel model.
And you can imagine also combining a
pronunciation-based model with a noisy
channel model and modern models of the
channel. In the last decade or so allow a
number of kind of improvements like this.
So incorporating a pronunciation component
into the channel model is one, and we
might also want to allow richer edits. So
not just single letter edits, but kinda
edits like a p, ph being incorrectly typed
as an f. Or, very common error, it's not
that all e's are mistakenly typed as a's,
but that the sequence -ent is likely to be
mistyped as -ant. So, a couple of
different improvements that a state of the
art system might have in the channel
model. And in fact, we could consider a
very large number of factors that could
influence the probability of a misspelling
given a word the channel model. So we've
talked about the source letter or the
target letter. And we've talked about, you
know, maybe one surrounding letter. But we
could look at more surrounding letters, or
we could look at the position in the word.
Maybe, some errors happen in the middle of
the word, some errors happen at the end.
We might explicitly model the keyboard,
and talk about nearby keys on the
keyboard. Or homology, we're likely to
mistype a, a, a word with our left hand,
third finger, by using our right hand
third finger. So, so a key which is on the
same finger on the alternate hand is
homologous. Or, again, we might use
pronunciations. We might use these kind of
likely morpheme transformations we talked
about, in the last slide. Lots of possible
factors that could influence the proba-,
this channel model. Here's a picture of
one of them, a keyboard. So we might wanna
say that R and W are likely mis,
mistypings for E and so on if we're on a,
some kind of a, a phone keyboard. So
combining all these different factors is
often done with a classifier-based model.
So the classifier-based model is an
alternative way of doing real-word
spelling correction. And here we, instead
of just two models, a channel model and a
language model, we might take those two
and a number of other models and combine
them in a big classifier. We'll talk about
classifiers In the next lecture and so for
example if we had a specific pair like
whether and weather, commonly confused
real word confusions. We might look at
features like, well is the word cloudy.
You know, window of plus or minus ten
words, or am I followed by the word two in
some and then some verb. So, if I, the
word, cloudy is, is nearby me I'm probably
the word weather. If I'm followed by two
verb, I'm probably the word whether, so,
whether to go, whether to say, whether to
do, is probably this whether. Similarly if
I'm followed by or not, then I'm probably
this weather. So each of these features.
Plus the language model plus the channel
model could be combined into one
classifier that could make a decision, I
mean we might build separate classifier
for each possible likely pair of words. So
in summary real words done in correction
can be done with the same noisy channel
algorithm that's used for non words
spellings correction but we can also use a
classifier based approach. We combine a
lot of features and build classifiers for
a very frequent kinds of errors we like to
model explicitly.
