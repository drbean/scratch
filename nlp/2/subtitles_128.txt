
How do we estimate these end gram
probabilities? Let's look at bigram
probabilities. The maximum likelihood
estimate for a bigram probability, the
probability of a word I, given the
previous word I-1, we just estimate by
counting. We count, how many times did
word I-1 and I occur together? And divide
it by how many times word I-1 occurs. So
it's like saying, of all the times that we
saw word I-1, how many times was it
followed by word I? And we'll use the
notation count sometimes. And sometimes,
we'll, for simplification, we'll just
refer to a C, we'll use C for count? So,
the joint count of word I-1 and I, divided
by the count of word I-1. So let's walk
through an example. So we have again our
equation, the probability of word I given
the word I minus one is the maximum like
the estimate is just. We'll write, I'll
write MLE. Estimated by the maximum
likelihood estimator as, as this count
over this count. And let's look at our,
let's look at, let's make up a corpus. So,
here's our simple corpus borrowed by Dr.
Seuss. So, we have three sentences, each
one starts with a special token start up
sentence, and end with a special token end
of sentence. And they're very short
sentences. I am Sam. Sam I am. I do not
like green eggs and ham. Let's compute
some language model probabilities from
this small corpus. So, first probability
of I given the start symbol, so that's
computed as count of start symbol. Comma
i. With a big I over count of start
symbol. So. I is the, follows the start
symbol twice, one, two, and the start
symbol occurs three times, one, two,
three. So that probability is two-thirds
4.67. So that's the probability of I given
start. And you can see common examples of
lots of different probabilities here. So
for example, let's just pick another one
at random. The probability of Sam given
the word am, how many times does am, Sam
occur? It occurs once. So that's one.
[sound]. And the denominator is, how many
times does am occur? And that occurs
twice. So that's gonna be one over two. So
here's our probability of [inaudible], and
so on. So let's look at a larger corpus in
order to get some more realistic counts.
And the corpus we're using here was
collected from a dialog system that
answered questions about restaurants in
the city of Berkeley, California. And
here's the kind of sentences that were in
this corpus: Can you tell me about any
good Cantonese restaurants close by? Or,
mid-price Thai food is what I'm looking
for. Tell me about Chez Panisse. Now I
hear some sentences, and let's, let's
compute some engrams based on these
sentences. So first, let's start with the
raw bi-gram counts from the small corpus
of just under 10,000 sentences. So, what
I'm showing you here is a bigram count
table, so here is the word I is followed
by the word I five times. The word I is
followed by the word want 827 times, the
word want is followed by the word to 680
times, the word to is followed by the word
eat 686 times and we've just put some
sample word appear, I picked I want to eat
Chinese food lunch [inaudible], just to
show you some words that might occur
together in a sentence and some other word
and you can see a lot of these words, a
lot of these probabilities are zero, a lot
of these [inaudible], I'm sorry are zero,
because just happened in this small data
set that want was never followed by want.
So that's this zero here or Chinese was
never followed by the word to here. Okay.
So in order to turn these counts into
probabilities all we have to do is
normalize by a unigram count. Because
remember the probability of a word I given
the word I minus one. Is the count of word
I - one. Word I over the count of word I
-one. So, we need to divide these joint
counts of the two words, by the count of
the previous word. Here's the uni-gram
counts we're going to need to compute
those probabilities. So, here's the count
if I, it's 25/33. Here's the count of E,
it's 7/46. And using the equation we can
now compute the bi-gram probability. The
probability for example of two given want
or likely. Given that the previous word
was want that the next word is two. And
it's pretty likely. For example.66. But
notice that things with counts of zero
still have probabilities of zero. Lots of
things have zeros here. So now, that we've
computed all of our bio-gram capabilities,
we can now estimate the probability of the
sentence, that's our goal for language
modeling. Simply by multiplying together
all the component probabilities. So the
probability of start of sentence I want
English food in the sentence is,
probability of I given start, times the
probability of want given I, times the
probability of English given want, food
given English, start given food and so on.
What kinds of knowledge are expressed by
these bigram probabilities. Why, for
example is the probability of English
given want lower than the probability of
Chinese given want. Well, probably that's
because Chinese food is more popular and
more people are gonna ask about it and so
wanting Chinese is more likely than want
in English. So that's a fact about the
world. It's a fact about cuisines not so
much a fact about English. What about the
probability of two given one being so
high. Now that's a fact about grammar.
That's a fact that the verb want, in
English, requires an infinitive after it.
So want as infinitives, and that's a
grammatical fact. [sound]. So that's
grammar, we'll put grammar. And what about
the probability that want, given the
previous word is spend, is zero? So that
zero seems to be caused by a grammatical
fact, spend want is two verbs in a row,
that kind of verb doesn't seem to be
grammatically possible in English. So that
zero is caused by a grammatical
disallowing. How about this zero? What's
the, why is the probability of food
following 2-0? Now here, this zero is a
contingent zero. This, you could imagine a
sentence that has two food in it. I'd like
you to stop and think about a sentence
like that yourself. Good. It just happens
that such a sentence never occurred in the
training data. So this is a contingency
row, where this is a structural zero. All
right, let's move on. In practice. We
don't keep these probabilities in the form
of probabilities, in fact we keep them in
the form of log probabilities and
[inaudible], there are two reasons for
this, one is, we is can avoid under flow,
if you think about it if you have a very
long sentence and you multiplying together
twenty or 30 or 40, little to tiny
probabilities [inaudible], should
[inaudible], a very small number less then
zero, when you multiply. With so many
small numbers, you get a very small number
that often ends up with arithmetic
underflow, in the computation. And so, we
want to avoid this kind of underflow. And
it turns out that adding is faster than
multiplying anyway. So, again, instead of
multiplying four probabilities we'll, in
general, just add four live probabilities.
Then we're going to store our language
models in logs. There are a number of
publicly available language modeling tool
kits. One of them. S R I L M. And you can download S.R.I.L.M. Another publicly
available resource is the Google N-Grams
release, this has been out for, for over
five years now. Google released a trillion
word corpus. Over a trillion five grams
and thirteen million unique words. So a
huge data set which, you can download and
use for your, any kind of N-gram
applications you'd like to use. So here's
an example, some of the data from the
Google N-Grams release. Some four-gram
counts for four-grams beginning with "serve
as the" and words beginning with I-n. So
this is a very big corpus, and, and you
can see that "serve as the indication"
occurred 72 times on this web corpus. And
you have more information on the, Google
web corpus there. Another publicly
available corpus is the Google book N-gram
corpus. Let's look at that. So this corpus
lets you plot counts of words in Google
books. And a number of corpora are
available for American English, British
English, Chinese, French, and German, and,
various kinds of corpora, which can all be downloaded
